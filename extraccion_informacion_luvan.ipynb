{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b70f19f8",
   "metadata": {},
   "source": [
    "<br />\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://www.davivienda.com/wps/portal/personas/nuevo\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/b/b1/Davivienda_logo.svg/1200px-Davivienda_logo.svg.png\" alt=\"Logo\" width=\"300\" height=\"100\">\n",
    "  </a>\n",
    "\n",
    "  <h2 align=\"center\"> Prueba Técniva Davivienda - Profesional I Data no Estructurada</h2>\n",
    "  <h2 align=\"center\"> Notebook de Implementación</h2>\n",
    "  <h4 align=\"center\"> Luvan Tabares </h4>\n",
    "    \n",
    "***\n",
    "\n",
    "  <p align=\"center\">\n",
    "    Realizar la oferta correcta en el momento adecuado es una de las principales estrategias de mercadeo que de manera comprobada aumentan la efectividad de las campañas comerciales. Esta estrategia es conocida como Siguiente Mejor Oferta (NBO) y es generalmente responsabilidad de las áreas de analítica de clientes dado que se construye a partir de modelos analíticos predictivos. En este estudio, se construyeron dos modelos de NBO de productos de crédito para clientes del Banco Davivienda. El primer modelo consistió en un Modelo Experto basado en reglas de negocio y asociaciones estadísticas de las variables independientes y la variable objetivo. El segundo modelo consistió en el entrenamiento de una red neuronal multicapa o Fully Connected. El estudio se basó en una base de datos de clientes del Banco Davivienda y sus aperturas de productos de crédito durante un semestre.  </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "* [1. EDA - Análisis Exploratorio de Datos](#topic1)\n",
    "* [2. Extracción de Nombres](#topic2)\n",
    "* [3. Extracción de Fechas](#topic3)\n",
    "* [4. Extracción de Montos](#topic4)\n",
    "* [5. Extracción de Ciudades](#topic5)\n",
    "* [6. Resultados Finales y Conclusiones](#topic6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b1ab4",
   "metadata": {},
   "source": [
    "### Instalación de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78300d73",
   "metadata": {},
   "source": [
    "Comandos necesarios para la instalación de las librerías utilizadas en este documento, se utilizó el modelo de lenguaje en español grande de spacy para mejorar la detección de las entidades requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7633af49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pandas==1.2.3\n",
    "# conda install -c conda-forge spacy==3.4.4\n",
    "# conda install -c conda-forge numpy\n",
    "# !python -m spacy download es_core_news_lg\n",
    "\n",
    "# !pip install --upgrade numpy --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572216f7",
   "metadata": {},
   "source": [
    "## Librerías necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5866d0",
   "metadata": {},
   "source": [
    "Las librerías utilizadas serán 'pandas' para el manejo de los archivos csv y su texto, 'spacy' para la tokenización y todos los procesos de nlp que se apliquen, librería 're' para el uso de expresiones regulares, 'datetime' para la manipulación del formato de fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f4b3ce9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8bbdadaec6cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# set library-specific custom warning handling before doing anything else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/spacy/errors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mErrorsWithCodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/spacy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mcatalogue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_importlib_metadata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimportlib_metadata\u001b[0m  \u001b[0;31m# type: ignore[no-redef]    # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizer\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/thinc/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigValidationError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormal_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglorot_uniform_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigure_normal_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalCrossentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2Distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCosineDistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceCategoricalCrossentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/thinc/initializers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFloatsXd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mShape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/thinc/backends/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcupy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCupyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmps_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMPSOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/thinc/backends/cupy_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_custom_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeviceTypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/thinc/backends/numpy_ops.pyx\u001b[0m in \u001b[0;36minit thinc.backends.numpy_ops\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from datetime import datetime\n",
    "nlp = spacy.load('es_core_news_lg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93701f5",
   "metadata": {},
   "source": [
    "## 1. **EDA - Análisis Exploratorio de Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870d9c6",
   "metadata": {},
   "source": [
    "Comenzar con el análisis de la información suministrada, importar el documento teniendo en cuenta el tipo de separador y dar un vistazo al documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('documentos (2).csv', '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5ca40",
   "metadata": {},
   "source": [
    "Se observa un documento de dos columnas y 499 filas, dónde la primer columna (indx 0) es el document_name o nombre del documento, que son números asociados a cada texto, y la segunda columna (indx 1) es la que contiene el texto que nos interesa procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac3b3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8787d9",
   "metadata": {},
   "source": [
    "Al analizar uno de los textos se observa que se encuentra la información de interés como nombre, monto, fecha y ciudad, el orden de estos no siempre es así, teniendo casos en los que el nombre se encuentra al final o no existe una ciudad de residencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc3b3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['document'][1] # Exploramos uno de los textos a analizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8788ada",
   "metadata": {},
   "source": [
    "## Estudio de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2739c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf86840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449c34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d87a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4e4f63b",
   "metadata": {},
   "source": [
    "## Funciones necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d8980",
   "metadata": {},
   "source": [
    "Dentro de la teoría básica de procesamiento de lenguaje natural nos encontraremos con las siguientes opciones de preprocesamiento:\n",
    "\n",
    "1.Tokenización\n",
    "2.Convertir texto a minúsculas \n",
    "3.Remover palabras de parada/artículos (Stop Words)\n",
    "4.Remover puntuación\n",
    "5.Estemización\n",
    "6.Lematización\n",
    "\n",
    "Para la tokenización se usará la librería 'spacy' y siempre se necesitará tokenizar el texto para su análisis, las otras opciones de preprocesamiento pueden ser usadas o no dependiendo de cómo se comporte el modelo, ya que para el análisis de este documento se observó que convertir a minúsculas llevaba a un error mayor en la identificación de las palabras (NER) y para cierta extracción de información se pueden usar diferentes combinaciones de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40640382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.text for token in doc if not token.like_url])\n",
    "\n",
    "def remove_stop(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.text for token in doc if not token.is_stop])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef5ab3c",
   "metadata": {},
   "source": [
    "Para este caso de análisis se observó que remover puntuación, convertir a minúsculas y lematizar empeoraban la capacidad de detección de entidades (NER) de spacy, muy probablemente por que para la detección analiza si las palabras empiezan por letra mayúscula y la puntuación y no lematizar le ayudan a detectar mejor el contexto. No se contempló usar estemización ya que conlleva a la creación de palabras que no existen el idioma, en un intento de obtener la raíz de la palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe00c71",
   "metadata": {},
   "source": [
    "# 2. Extracción de Nombres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abbe0f",
   "metadata": {},
   "source": [
    "Como primer paso de extracción se realizará la extracción de los nombres presentes en el documento, la librería spacy los detectará con la etiqueta 'PER' y analizando los documentos se observó que los nombres empiezan por letra mayúscula y son nombre compuestos, es decir dos nombres dos apellidos, pero se tuvo en cuenta el caso de personas con un sólo nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8832199",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('hola')\n",
    "nlp.Defaults.stop_words.add('estimado')\n",
    "nlp.Defaults.stop_words.add('estimada')\n",
    "nlp.Defaults.stop_words.add('estimadísimo')\n",
    "nlp.Defaults.stop_words.add('estimadísima')\n",
    "nlp.Defaults.stop_words.add('don')\n",
    "nlp.Defaults.stop_words.add('doña')\n",
    "nlp.Defaults.stop_words.add('amigo')\n",
    "nlp.Defaults.stop_words.add('amiga')\n",
    "nlp.Defaults.stop_words.add('esperamos')\n",
    "nlp.Defaults.stop_words.add('queridísimo')\n",
    "nlp.Defaults.stop_words.add('queridísima')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd55708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_nlp\"] = df[\"document\"].str.replace('oiga', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d2a77",
   "metadata": {},
   "source": [
    "En las dos celdas de arriba se empezó a agregar palabras que interferían con la detección de spacy y que se podrían considerar como Stop Words para este análisis ya que no nos suministran información relevante, la palabra oiga no estaba siendo eliminada usando la remoción de stop words por lo que se utilizó un replace para eliminarla forzosamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc92bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_nlp\"] = df[\"text_nlp\"].apply(remove_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d24ce",
   "metadata": {},
   "source": [
    "La eliminación de la palabra 'oiga' también me permite crear una nueva columna para la manipulación de la información sin cambiar el texto original y poder llamarlo de nuevo en caso de ser necesario, y ahora si aplico la remoción de Stop_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21cdff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "names4 = r'[A-ZÁÉÍÓÚÑa-záéíóúñ]+\\s[A-ZÁÉÍÓÚÑa-záéíóúñ]+\\s[A-ZÁÉÍÓÚÑa-záéíóúñ]+\\s[A-ZÁÉÍÓÚÑa-záéíóúñ]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names3 = r'[A-ZÁÉÍÓÚÑa-záéíóúñ]+\\s[A-ZÁÉÍÓÚÑa-záéíóúñ]+\\s[A-ZÁÉÍÓÚÑa-záéíóúñ]+'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56d6b6",
   "metadata": {},
   "source": [
    "Como se mencionó anteriormente consideré que existen dos nombres, compuestos y simples, dónde los compuestos tendrán 4 palabras que empizan por letra mayúscula y los simples es lo mismo pero con 3 palabras. Se tuvo en cuenta tíldes y la letra ñ propios de nuestro lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_names(text):\n",
    "            \n",
    "    doc = nlp(text)  \n",
    "\n",
    "    for ent in doc.ents:          \n",
    "        if ent.label_ == \"PER\":\n",
    "            if re.search(names4, ent.text) or re.search(names3, ent.text):\n",
    "                q = ent.text.replace('   carta', '').replace('¡ !', '').replace('   ', '')\n",
    "\n",
    "                name = q\n",
    "                                   \n",
    "                break\n",
    "            else:\n",
    "                name = 'No encontrado'\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3be421",
   "metadata": {},
   "source": [
    "La función extract_names recibirá el texto correspondiente a cada fila y aplicará tokenización (nlp(text)), al tokenizar se pueden usar las funciones de spacy y convierte el texto str a una lista con cada palabra o grupo de palabras tokenizadas, en dónde spacy le dará unas etiquetas o TAG's conociendo que la etiqueta de personas o nombres se llama 'PER' busco en la lista cada token que tenga la etiqueta PER y reviso si tienen la forma de un nombre simple o compuesto y realizo una limpieza final de algunos elementos que tomaba como parte de la etique. En caso de que no encuentre un token que satisfaga las condiciones devolverá 'No encontrado'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d4b49e",
   "metadata": {},
   "source": [
    "## 3. Extracción de Fechas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74a30a",
   "metadata": {},
   "source": [
    "Para la extracción de las fechas se observó mejor comportamiento al usar de nuevo el documento desde el principio, por lo que se reescribirá la columna 'text_nlp' del csv por una no procesada, cuyo único preprocesamiento será la eliminación de espacios alargados en el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_nlp\"] = df[\"document\"].str.replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c944e1",
   "metadata": {},
   "source": [
    "Se eliminan los espacios alargados en el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fechapat = r'([\\d]{4}(\\-|\\/|\\_)[\\d]{2}(\\-|\\/|\\_)[\\d]{2})|([\\d]{2}(\\-|\\/|\\_)[\\d]{2}(\\-|\\/|\\_)[\\d]{4})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977cdd4",
   "metadata": {},
   "source": [
    "Este es el regex propuesto para la detección de las fechas, revisando algunos documentos se observó que tenían dos formatos, AÑO-mes-día y mes/día/año, por lo que el regex detecta si es primer o segundo patrón y dependiendo de qué patrón muestre la fecha se procesará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(text):\n",
    "\n",
    "\n",
    "    match = re.search(fechapat, text)\n",
    "\n",
    "    \n",
    "    number = match.group()\n",
    "    \n",
    "    if number:\n",
    "        if \"-\" in number:\n",
    "\n",
    "            date = datetime.strptime(number, \"%Y-%m-%d\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            date = datetime.strptime(number, \"%m/%d/%Y\")\n",
    "\n",
    "\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "        day = date.day    \n",
    "\n",
    "\n",
    "        today = datetime(year, month, day) #(YEAR, MONTH, DAY)\n",
    "\n",
    "        fchas = f'{today:%Y}-{today:%m}-{today:%d}'\n",
    "    \n",
    "    else:\n",
    "        fchas = 'No encontrado'\n",
    "    \n",
    "    \n",
    "    \n",
    "       \n",
    "    return fchas\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499c89a",
   "metadata": {},
   "source": [
    "Este código analizará si en el texto existe un patrón de fecha anteriormente mencionado, si existe extraerá la fecha y será procesada dependiendo del formato, una vez se procesa se obtiene el año, el mes y el día usando datetime. Ahora que tenemos separado quién es año, mes y día se vuelve a usar datetime y se escribe con el formato solicitado AÑO-mes-día, en caso de no encontrar un patrón devolverá 'No encontrado'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496ea5e",
   "metadata": {},
   "source": [
    "## 4. Extracción de Montos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca2b20",
   "metadata": {},
   "source": [
    "Para la extracción de montos se utilizará el mismo texto que en el caso de fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd63f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpat = r'[\\d]{0,3}(\\,|\\.)?[\\d]{0,3}(\\,|\\.)?[\\d]{0,3}(\\,|\\.)?[\\d]{1,3}(\\,|\\.)[\\d]{1,2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4653d3",
   "metadata": {},
   "source": [
    "Se creó un regex que cumpliera con la condición de estar separado por puntos o comas y tener entre 0 a 3 dígitos por sección entre puntos y comas, es decir detectará números como 111,111,111.11 ó 1,111,111.11. Se mantuvo una restricción de terminar en punto ya que todos los documentos presentaban ese patrón, además de 2 cifras decimales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_money(text):\n",
    "\n",
    "\n",
    "    match = re.search(numpat, text)\n",
    "\n",
    "    if match:\n",
    "        number = match.group()\n",
    "\n",
    "        m = number.replace(',', '/').replace('.', ',').replace('/', '')\n",
    "\n",
    "        mone = m\n",
    "    else:\n",
    "        mone = 'No encontrado'\n",
    "    \n",
    "    return mone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da2339",
   "metadata": {},
   "source": [
    "Para extraer el monto es muy similar a extraer la fecha, se revisa si en el texto existe el patrón y si hay patrón se extrae ese número, posteriormente se cambian las comas por barra oblicua (slash), luego se reemplazan los puntos por comas y se eliminan los slash, para así entregar el monto en el formato solicitado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b6d48",
   "metadata": {},
   "source": [
    "## 5. Extracción de Ciudades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b07ed",
   "metadata": {},
   "source": [
    "En el caso de la extracción de ciudades por seguridad se volvió a tomar el documento original y se preprocesará eliminado espacios y algunas Stop Words indeseadas, no se eliminaron las Stop Words por el método anterior en nombres ya que la eliminación de las Stop Words comunes empeoraba la detección del spacy, por lo que se utilizó el replace para hacer una remoción de Stop Words un poco más específica, además que se notó una mejoría en la detección al aplicar el removedor de url al texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8fe1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_nlp\"] = df[\"document\"].str.replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda143cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_nlp\"] = df[\"text_nlp\"].str.replace('Nos', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_nlp\"] = df[\"text_nlp\"].str.replace('Initech', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72807dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_nlp\"] = df[\"text_nlp\"].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785467c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudad1 = r'([A-Z])([a-záéíóúñ]+)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d0596",
   "metadata": {},
   "source": [
    "Para el caso de comparar si lo encontrado es una ciudad lo ideal sería compararlo con una lista de todas las ciudades de colombia, pero por cuestiones de tiempo se decidió utilizar un regex que simplemente detectara una palabra que empezara por letra mayúscula ya que se observó que las ciudades son palabras simples o compuestas pero empiezan por una letra maýuscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959eee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_city(text):\n",
    "\n",
    "        doc = nlp(text) \n",
    "        \n",
    "        for ent in doc.ents:          \n",
    "            if ent.label_ == \"LOC\":\n",
    "                \n",
    "                q = ent.text\n",
    "                if q == 'Bogotá':\n",
    "                    q = 'No encontrado'\n",
    "                elif re.search(ciudad1, q) is None:\n",
    "                    q = 'No encontrado'\n",
    "\n",
    "                cet = q\n",
    "                       \n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        return cet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbfc872",
   "metadata": {},
   "source": [
    "La función extraerá el texto y lo tokenizará, luego podemos revisar si estos token son una 'LOC' que se refiera a las ciudades o lugares, posteriormente se analiza si la ciudad es Bogotá o no, aunque hacer esta aseveración realmente sería un error se observó en los documentos que todas las cartas eran enviadas desde Bogotá por lo que Bogotá se podría considerar una Stop Word, lo ideal sería comparar con una lista de ciudades de colombia, además de revisar si hay más de 1 token 'LOC' en el texto, si sólo hay un token 'LOC' muy probablemente sea la ciudad de la empresa y no la ciudad de la persona a la cuál se le está cobrando. Entonces aquí se analizó como que en caso de la 'LOC' sea Bogotá lo tome como 'No encontrado' y en caso de que no encuentre token con la etiqueta 'LOC' devuelva 'No encontrado'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88d0c1",
   "metadata": {},
   "source": [
    "## Resultados Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nombre_cliente'] = df['text_nlp'].apply(extract_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5548932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['fecha'] = df['text_nlp'].apply(extract_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cdf682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['monto'] = df['text_nlp'].apply(extract_money)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ecc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ciudad'] = df['text_nlp'].apply(extract_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8c2f2",
   "metadata": {},
   "source": [
    "Aquí se llaman cada una de las funciones y se crea una nueva columna con dicha información"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78488f80",
   "metadata": {},
   "source": [
    "## Arreglo de la tabla final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abaaa7",
   "metadata": {},
   "source": [
    "Para terminar, el archivo csv quedó con el orden incorrecto ya que se llamaron las funciones en el orden presentado, con el código de abajo se va a cambiar de posición la última columna de ciudades a su posición además de renombrar las columnas suministradas por el formato deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd547e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['text_nlp'])\n",
    "column_name = 'ciudad'\n",
    "new_pos = 3\n",
    "column_names = list(df.columns)\n",
    "column_names.remove(column_name)\n",
    "column_names.insert(new_pos, column_name)\n",
    "df = df[column_names]\n",
    "df = df.rename(columns={'document_name': 'nombre_documento'})\n",
    "df = df.rename(columns={'document': 'texto'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f432d6",
   "metadata": {},
   "source": [
    "Presentamos la tabla dónde se observa que quedó con el formato solicitado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac29df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd2bee",
   "metadata": {},
   "source": [
    "Y se exporta el archivo csv con el separador '|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8500d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('resultados.csv', sep = '|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
